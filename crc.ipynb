{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5JC8JQa505D"
      },
      "outputs": [],
      "source": [
        "!wget -nc -q https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat\n",
        "!wget -nc -q https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhbcJogFWnY1"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import geopy.distance\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame\n",
        "import powerlaw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reading and converting data format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by reading the airports file to a pandas Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTrNTZEYgDWB"
      },
      "outputs": [],
      "source": [
        "airportsDF = pd.read_csv('airports.dat', header=None, names=['id', 'name', 'city', 'country', 'IATA', 'ICAO', 'lat', 'lon', 'alt', 'timezone', 'DST', 'Tz', 'type', 'source'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From there, we create a directed graph and add all the airports as nodes to it (except airports with no IATA identifier, which are ignored).\n",
        "This is because we will use this identifiers later to associate this data with the routes data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "G = nx.DiGraph()\n",
        "\n",
        "for index, row in airportsDF.iterrows():\n",
        "    if row['IATA'] == '\\\\N':\n",
        "        continue\n",
        "    G.add_node(row['IATA'], pos=(row['lat'],row['lon']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our nodes are identified by their IATA identifiers and have a corresponding position (corresponding to their real geographic location) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(G.nodes['LIS'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have a graph with nodes but no edges"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiSrc2C4hECI",
        "outputId": "dc0876ff-9aea-46f9-bece-778dc9682462"
      },
      "outputs": [],
      "source": [
        "print(\"Node count: \" + str(len(G.nodes)))\n",
        "print(\"Edge count: \" + str(len(G.edges)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we read the routes data and add each one as an edge of our graph previously created\n",
        "This routes are directed, meaning that if an airline operates services from A to B and from B to A, both A-B and B-A are listed as diferent edges\n",
        "We also add weight to this edges, the distance between airports in km."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "routesDF = pd.read_csv('routes.dat', header=None, names=['airline', 'airlineID', 'source', 'sourceID', 'dest', 'destID', 'codeshare', 'stops', 'equipment'])\n",
        "\n",
        "for index, row in routesDF.iterrows():\n",
        "    if(row['source'] not in G.nodes or row['dest'] not in G.nodes):\n",
        "        continue\n",
        "    dist = geopy.distance.distance((G.nodes[row['source']]['pos']), (G.nodes[row['dest']]['pos'])).km\n",
        "    G.add_edge(row['source'], row['dest'], weight=dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, we take a route from Lisbon to Cologne to show this representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list(G.edges('LIS',data=True))[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see that our graph has nodes and edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Node count: \" + str(len(G.nodes)))\n",
        "print(\"Edge count: \" + str(len(G.edges)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleaning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now \"clean\" our graph, removing the nodes which have either 0 incoming flights or 0 outgoing flights (or both).\n",
        "We iterate the graph until no more changes are made. This is because after removing one node, another one might lose the only incoming or outgoing route it had previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i=0\n",
        "while i == 0:\n",
        "  i = 1\n",
        "  H = G.copy()\n",
        "  for node in H.in_degree:\n",
        "    if node[1] == 0:\n",
        "      i = 0\n",
        "      G.remove_node(node[0])\n",
        "      \n",
        "  H = G.copy()\n",
        "  for node in H.out_degree:\n",
        "    if node[1] == 0:\n",
        "      i = 0\n",
        "      G.remove_node(node[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We end up with a significant reduction in nodes and lose barely any edges, obtaining what we wanted and removing unutilized airports from the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Node count: \" + str(len(G.nodes)))\n",
        "print(\"Edge count: \" + str(len(G.edges)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysing network metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see that the Frankfurt airport, Charles de Gaulle airport in Paris and Amesterdam are the nodes with the highest degree, with around 450 incoming or outgoing routes each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
        "print(nodes[0:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also see that the majority of airports have a very low degree, resulting in a near zero density in our graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graph density\n",
        "nx.density(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the degree distribution we verify that almost 800 airports (of the near 3000) have a degree of only 2, meaning only one incoming route and one outgoing one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Degree Distribution\n",
        "degrees = [G.degree(n) for n in G.nodes()]\n",
        "plt.hist(degrees, bins=np.logspace(np.log10(2),np.log10(max(degrees)), 20))\n",
        "plt.xscale(\"log\")\n",
        "plt.title(\"Histogram of Degree Distribution\")\n",
        "plt.ylabel(\"Node Count\")\n",
        "plt.xlabel(\"Degree Value\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cumulative distribution also proves this point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "degrees = [G.degree(n) for n in G.nodes()]\n",
        "degree_sequence = sorted(degrees, reverse=True) # degree sequence\n",
        "degreeCount = collections.Counter(degree_sequence)\n",
        "deg, cnt = zip(*degreeCount.items())\n",
        "cs = np.cumsum(cnt)\n",
        "plt.loglog(deg, cs, 'bo')\n",
        "plt.title(\"Cumulative Distribution plot\")\n",
        "plt.ylabel(\"Sample with value > Degree\")\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "degrees = [G.degree(n) for n in G.nodes()]\n",
        "degree_counts = Counter(degrees)\n",
        "print(degree_counts)\n",
        "\n",
        "x, y = zip(*degree_counts.items())\n",
        "\n",
        "plt.xlabel('Degree')\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.ylabel('Node count')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.scatter(x, y, marker='.')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Degree centrality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now calculate some metrics of the network, starting with the degree centrality\n",
        "As seen before, Frankfurt is the airport with the highest degree.\n",
        "A military base in Greenland is the one of the many airports with only degree 2, resulting in a low degree centrality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute graph degree centrality\n",
        "dc = nx.degree_centrality(G)\n",
        "\n",
        "max_dc = max(dc, key=dc.get)\n",
        "print(\"Airport with the max degree centrality: \" + max_dc)\n",
        "print(\"Value: \" + str(dc[max_dc]))\n",
        "\n",
        "min_dc = min(dc, key=dc.get)\n",
        "print(\"Airport with the min degree centrality: \" + min_dc)\n",
        "print(\"Value: \" + str(dc[min_dc]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvector centrality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the eigenvector centrality we get that Amsterdam airport has the highest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute eigen vector centrality\n",
        "ec = nx.eigenvector_centrality(G)\n",
        "\n",
        "max_ec = max(ec, key=ec.get)\n",
        "print(\"Airport with the max eigenvector centrality: \" + max_ec)\n",
        "print(\"Value: \" + str(ec[max_ec]))\n",
        "\n",
        "min_ec = min(ec, key=ec.get)\n",
        "print(\"Airport with the min eigenvector centrality: \" + min_ec)\n",
        "print(\"Value: \" + str(ec[min_ec]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closeness centrality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the closeness centrality we get that Frankfurt airport has the highest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute closeness centrality\n",
        "cc = nx.closeness_centrality(G, distance='weight')\n",
        "\n",
        "max_cc = max(cc, key=cc.get)\n",
        "print(\"Airport with the max closeness centrality: \" + max_cc)\n",
        "print(\"Value: \" + str(cc[max_cc]))\n",
        "\n",
        "min_cc = min(cc, key=cc.get)\n",
        "print(\"Airport with the min closeness centrality: \" + min_cc)\n",
        "print(\"Value: \" + str(cc[min_cc]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Harmonic centrality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute harmonic centrality\n",
        "hc = nx.harmonic_centrality(G, distance='weight')\n",
        "\n",
        "max_hc = max(hc, key=hc.get)\n",
        "print(\"Airport with the max harmonic centrality: \" + max_hc)\n",
        "print(\"Value: \" + str(hc[max_hc]))\n",
        "\n",
        "min_hc = min(hc, key=hc.get)\n",
        "print(\"Airport with the min harmonic centrality: \" + min_hc)\n",
        "print(\"Value: \" + str(hc[min_hc]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sort cc and hc by dictionary key\n",
        "cc = dict(sorted(cc.items()))\n",
        "hc = dict(sorted(hc.items()))\n",
        "\n",
        "# Correlation matrix between Closerness Centrality and Harmonic Centrality\n",
        "cc_list = list(cc.values())\n",
        "hc_list = list(hc.values())\n",
        "# normalize\n",
        "cc_list = (cc_list - np.mean(cc_list))/np.std(cc_list)\n",
        "hc_list = (hc_list - np.mean(hc_list))/np.std(hc_list)\n",
        "print(np.corrcoef(cc_list, hc_list))\n",
        "print(cc_list)\n",
        "print(hc_list)\n",
        "# plot scatter plot\n",
        "plt.scatter(cc_list, hc_list)\n",
        "plt.xlabel(\"Closeness Centrality values (normalized)\")\n",
        "plt.ylabel(\"Harmonic Centrality values (normalized)\")\n",
        "m, b = np.polyfit(cc_list, hc_list, 1)\n",
        "plt.plot(cc_list, m*cc_list + b , color='red', label=\"Linear Regression\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Betweenness centrality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the betweenness centrality we get that Charles de Gaulle airport has the highest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute betweenness centrality\n",
        "bc = nx.betweenness_centrality(G)\n",
        "\n",
        "max_bc = max(bc, key=bc.get)\n",
        "print(\"Airport with the max betweenness centrality: \" + max_bc)\n",
        "print(\"Value: \" + str(bc[max_bc]))\n",
        "\n",
        "min_bc = min(bc, key=bc.get)\n",
        "print(\"Airport with the min betweenness centrality: \" + min_bc)\n",
        "print(\"Value: \" + str(bc[min_bc]))\n",
        "\n",
        "# sort the betweenness centrality\n",
        "sorted_bc = sorted(bc.items(), key=lambda x: x[1], reverse=True)\n",
        "print(sorted_bc[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Page rank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the page rank we get that Charles de Gaulle airport has the highest value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute pagerank\n",
        "pr = nx.pagerank(G)\n",
        "\n",
        "max_pr = max(pr, key=pr.get)\n",
        "print(\"Airport with the max page rank: \" + max_pr)\n",
        "print(\"Value: \" + str(pr[max_pr]))\n",
        "\n",
        "min_pr = min(pr, key=pr.get)\n",
        "print(\"Airport with the min page rank: \" +min_pr)\n",
        "print(\"Value: \" + str(pr[min_pr]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the clustring we get that Goroka airport has the highest value. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute clustering coefficient\n",
        "cl = nx.clustering(G)\n",
        "\n",
        "max_cl = max(cl, key=cl.get)\n",
        "print(\"Airport with the max clustering: \" + max_cl)\n",
        "print(\"Value: \" + str(cl[max_cl]))\n",
        "\n",
        "min_cl = min(cl, key=cl.get)\n",
        "print(\"Airport with the min clustering: \" + min_cl)\n",
        "print(\"Value: \" + str(cl[min_cl]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Strongly connected components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the networkx function to get us the strongly connected components, we verify that out graph has 7.\n",
        "One of them has almost every node and the rest are small, due to this we'll only consider the biggest one in some future steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate strongly connected components\n",
        "sccs = nx.strongly_connected_components(G)\n",
        "print([len(Gc) for Gc in sccs])\n",
        "\n",
        "sccs = nx.strongly_connected_components(G)\n",
        "sccs_graphs = (G.subgraph(c) for c in sccs)\n",
        "\n",
        "largest_scc = list(sccs_graphs)[0]\n",
        "print(len(largest_scc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classifying regime of network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "degrees = G.degree()\n",
        "print(degrees)\n",
        "sum_of_degrees = 0\n",
        "for x in degrees:\n",
        "    sum_of_degrees += x[1]\n",
        "\n",
        "degree_average = sum_of_degrees / len(degrees)\n",
        "print(\"degree average: \" + str(degree_average))\n",
        "\n",
        "log_degree = math.log(degree_average)\n",
        "log_n = math.log(len(degrees))\n",
        "\n",
        "print(\"log n: \" + str(log_n))\n",
        "print(\"log degree: \" + str(log_degree))\n",
        "\n",
        "\n",
        "print(\"log n / log degree: \" + str(log_n / log_degree))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before having a strongly connected component, calculating the average shortest path and the diameter of the network was not possible since there were some infinite distances, now we can do it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Average shortest path lenght and diameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_path_length = nx.average_shortest_path_length(largest_scc)\n",
        "print(avg_path_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nx.diameter(largest_scc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dists = dict(nx.all_pairs_shortest_path_length(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(dists['LIS'])\n",
        "seen_nodes = []\n",
        "teste = {}\n",
        "for node in G.nodes():\n",
        "    seen_nodes.append(node)\n",
        "    \n",
        "    for k, v in dists[node].items():\n",
        "        if k not in seen_nodes:\n",
        "            teste[v] = teste.get(v, 0) + 1\n",
        "            \n",
        "print(teste)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(teste.values()))\n",
        "print(list(teste.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_list = list(range(1,13))\n",
        "y_list = list(teste.values())\n",
        "y_list_norm = []\n",
        "size = sum(list(teste.values()))\n",
        "for y in y_list:\n",
        "    y_list_norm.append(y / size)\n",
        "\n",
        "cumulative = np.cumsum(y_list_norm)\n",
        "point = 0.9 * sum(y_list_norm)\n",
        "\n",
        "plt.plot(x_list, [point]*len(x_list), \"--\")\n",
        "plt.plot([5]*len(x_list), cumulative, \"--\")\n",
        "plt.plot(x_list, cumulative, label=\"CDF\")\n",
        "plt.plot(x_list, y_list_norm, color=\"red\", label=\"PDF\") \n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the data on a map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot each airport in a map and give each dot some coloring and size according to its centrality.\n",
        "Having created a function that receives which centrality to be used, the list of nodes to be plotted and a threshold (values lower that this threshold won't be displayed), we can plot the maps for each of the centralities calculated before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotMap(label, centrality_dict, list_nodes, threshold):\n",
        "    newAirportsDF = pd.DataFrame(columns=['id', 'name', 'city', 'country', 'IATA', 'ICAO', 'lat', 'lon', 'alt', 'timezone', 'DST', 'Tz', 'type', 'source'])\n",
        "    \n",
        "    # get max and min to normalize\n",
        "    max_value = max(centrality_dict.values())\n",
        "    min_value = min(centrality_dict.values())\n",
        "\n",
        "    # from the list given, add the nodes that have a higher centrality value than the threshold\n",
        "    for node in list_nodes:\n",
        "        if(centrality_dict[node] > threshold):\n",
        "            x = airportsDF.loc[airportsDF['IATA'] == node]\n",
        "            x_norm = (centrality_dict[node] - min_value) / (max_value - min_value)\n",
        "            x['color'] = centrality_dict[node]\n",
        "            x['markersize'] = (x_norm **4) * 200\n",
        "            newAirportsDF = pd.concat([newAirportsDF, x])\n",
        "\n",
        "    geometry = [Point(xy) for xy in zip(newAirportsDF['lon'], newAirportsDF['lat'])]\n",
        "    gdf = GeoDataFrame(newAirportsDF, geometry=geometry)\n",
        "\n",
        "    # #this is a simple map that goes with geopandas\n",
        "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "    gdf.plot(column=\"color\", ax=world.plot(figsize=(20, 12)), marker='o', markersize=\"markersize\", cmap='rainbow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ignores a warning that is not worrying in this case and creates a lot of noisy output\n",
        "import warnings\n",
        "from pandas.errors import SettingWithCopyWarning\n",
        "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Degree centrality map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotMap(\"Degree centrality\", dc, G.nodes, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenvector centrality map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotMap(\"Eigenvector centrality\", ec, G.nodes, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Closeness centrality map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotMap(\"Closeness centrality\", cc, G.nodes, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Betweenness centrality map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotMap(\"Betweenness centrality\", bc, G.nodes, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Page rank map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plotMap(\"Page rank\", pr, G.nodes, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assortativity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(nx.algorithms.assortativity.degree_pearson_correlation_coefficient(G))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "min_val, max_val = min(G.degree, key = lambda d: d[1])[1], max(G.degree, key = lambda d: d[1])[1]\n",
        "\n",
        "mapping = {x: x for x in range(max_val//9)}\n",
        "\n",
        "matrix = nx.algorithms.assortativity.degree_mixing_matrix(G, mapping=mapping)\n",
        "\n",
        "f = plt.figure(figsize=(19, 15))\n",
        "plt.matshow(matrix, fignum=f.number)\n",
        "cb = plt.colorbar()\n",
        "cb.ax.tick_params(labelsize=14)\n",
        "plt.title('Correlation Matrix', fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Average of neighbors' degrees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nodes = sorted(G.degree, key=lambda x: x[1])\n",
        "i=0\n",
        "degree = nodes[0][1]\n",
        "\n",
        "degrees = [degree]\n",
        "average_list = [[],]\n",
        "\n",
        "for node in nodes:\n",
        "    if node[1] != degree:\n",
        "        degree=node[1]\n",
        "        degrees.append(degree)\n",
        "        average_list[i] = int(sum(average_list[i])/len(average_list[i]))\n",
        "        average_list.append([])\n",
        "        i+=1\n",
        "    for neig in nx.all_neighbors(G, node[0]):\n",
        "        average_list[i].append(G.degree[neig])\n",
        "\n",
        "average_list[i] = int(sum(average_list[i])/len(average_list[i]))\n",
        "xpoints = np.array(degrees)\n",
        "ypoints = np.array(average_list)\n",
        "\n",
        "plt.xlabel(\"Degree\")\n",
        "plt.ylabel(\"Average degree of neighbors\")\n",
        "plt.plot(xpoints, ypoints)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Communities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "D = G.copy()\n",
        "print(len(D.nodes))\n",
        "print(len(D.edges))\n",
        "i=0\n",
        "while i == 0:\n",
        "  i = 1\n",
        "  H = D.copy()\n",
        "  for node in H.in_degree:\n",
        "    if node[1] < 5:\n",
        "      i = 0\n",
        "      D.remove_node(node[0])\n",
        "      \n",
        "  H = D.copy()\n",
        "  for node in H.out_degree:\n",
        "    if node[1] < 5:\n",
        "      i = 0\n",
        "      D.remove_node(node[0])\n",
        "\n",
        "print(len(D.nodes))\n",
        "nodes = sorted(D.degree, key=lambda x: x[1])\n",
        "print(nodes)\n",
        "print(len(D.edges))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Greedy algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "g = nx.algorithms.community.greedy_modularity_communities(D, resolution=2)\n",
        "print(len(g))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "newAirportsDF = pd.DataFrame(columns=['id', 'name', 'city', 'country', 'IATA', 'ICAO', 'lat', 'lon', 'alt', 'timezone', 'DST', 'Tz', 'type', 'source'])\n",
        "\n",
        "#generate a random color for each community and plot them in the map\n",
        "for community in g:\n",
        "    rand_color = randint(0,80)\n",
        "    for node in community:\n",
        "        x = airportsDF.loc[airportsDF['IATA'] == node]\n",
        "        x['color'] = rand_color\n",
        "        newAirportsDF = pd.concat([newAirportsDF, x])\n",
        "\n",
        "geometry = [Point(xy) for xy in zip(newAirportsDF['lon'], newAirportsDF['lat'])]\n",
        "gdf = GeoDataFrame(newAirportsDF, geometry=geometry)\n",
        "\n",
        "# this is a simple map that goes with geopandas\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "gdf.plot(column=\"color\", ax=world.plot(figsize=(20, 12)), marker='o', markersize=8, cmap='gist_ncar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Louvain method (best method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bp = nx.algorithms.community.louvain_communities(D)\n",
        "print(len(bp))\n",
        "print(bp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "newAirportsDF = pd.DataFrame(columns=['id', 'name', 'city', 'country', 'IATA', 'ICAO', 'lat', 'lon', 'alt', 'timezone', 'DST', 'Tz', 'type', 'source'])\n",
        "\n",
        "#generate a random color for each community and plot them in the map\n",
        "for community in bp:\n",
        "    rand_color = randint(0,80)\n",
        "    for node in community:\n",
        "        x = airportsDF.loc[airportsDF['IATA'] == node]\n",
        "        x['color'] = rand_color\n",
        "        newAirportsDF = pd.concat([newAirportsDF, x])\n",
        "\n",
        "geometry = [Point(xy) for xy in zip(newAirportsDF['lon'], newAirportsDF['lat'])]\n",
        "gdf = GeoDataFrame(newAirportsDF, geometry=geometry)\n",
        "\n",
        "# this is a simple map that goes with geopandas\n",
        "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "gdf.plot(column=\"color\", ax=world.plot(figsize=(20, 12)), marker='o', markersize=8, cmap='gist_ncar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting degree distribution to a power law function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "degree_sequence = sorted([d for n, d in G.degree()], reverse=True) # used for degree distribution and powerlaw test\n",
        "\n",
        " # Power laws are probability distributions with the form:p(x)∝x−α\n",
        "fit = powerlaw.Fit(degree_sequence)\n",
        "\n",
        "print(fit.power_law.alpha)\n",
        "\n",
        "fig2 = fit.plot_pdf(color='b', linewidth=2)\n",
        "fit.power_law.plot_pdf(color='g', linestyle='--', ax=fig2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Robustness analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#about 7 minutes to run\n",
        "max = len(G.nodes())\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "strongest_connection = 3190\n",
        "\n",
        "nodes = sorted(G.degree, key=lambda x: x[1], reverse=True)\n",
        "for number_removed in range(max):\n",
        "    F = G.copy()\n",
        "    number_of_edges = []\n",
        "    \n",
        "\n",
        "    for i in nodes[0: number_removed]:\n",
        "        F.remove_node(i[0])\n",
        "\n",
        "    sccs = sorted(nx.strongly_connected_components(F), key=lambda t: len(t), reverse=True)\n",
        "    x += [number_removed/max]\n",
        "    y += [len(sccs[0])/strongest_connection]\n",
        "\n",
        "\n",
        "print(x)\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#about 6 minutes to run\n",
        "from random import shuffle\n",
        "max2 = len(G.nodes())\n",
        "x2 = []\n",
        "y2 = []\n",
        "\n",
        "strongest_connection2 = 3190\n",
        "\n",
        "\n",
        "nodes = list(G.degree)\n",
        "shuffle(nodes)\n",
        "\n",
        "for number_removed in range(max2):\n",
        "    F = G.copy()\n",
        "    number_of_edges = []\n",
        "    \n",
        "\n",
        "    for i in nodes[0: number_removed]:\n",
        "        F.remove_node(i[0])\n",
        "\n",
        "    sccs = sorted(nx.strongly_connected_components(F), key=lambda t: len(t), reverse=True)\n",
        "    x2 += [number_removed/max2]\n",
        "    y2 += [len(sccs[0])/strongest_connection2]\n",
        "\n",
        "\n",
        "print(x2)\n",
        "print(y2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#about 7 minutes to run\n",
        "max3 = len(G.nodes())\n",
        "x3 = []\n",
        "y3 = []\n",
        "\n",
        "strongest_connection3 = 3190\n",
        "\n",
        "bc = nx.betweenness_centrality(G)\n",
        "\n",
        "nodes = sorted(bc.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for number_removed in range(max3):\n",
        "    F = G.copy()\n",
        "    number_of_edges = []\n",
        "    \n",
        "\n",
        "    for i in nodes[0: number_removed]:\n",
        "        F.remove_node(i[0])\n",
        "\n",
        "    sccs = sorted(nx.strongly_connected_components(F), key=lambda t: len(t), reverse=True)\n",
        "    x3 += [number_removed/max3]\n",
        "    y3 += [len(sccs[0])/strongest_connection3]\n",
        "\n",
        "\n",
        "print(x3)\n",
        "print(y3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x3, y3)\n",
        "plt.xlabel('Number of removed nodes')\n",
        "plt.ylabel('SCCs')\n",
        "plt.title('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Studying the SI Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotMapSI(infected_dic, list_nodes):\n",
        "    newAirportsDF = pd.DataFrame(columns=['id', 'name', 'city', 'country', 'IATA', 'ICAO', 'lat', 'lon', 'alt', 'timezone', 'DST', 'Tz', 'type', 'source'])\n",
        "\n",
        "    # from the list given, add the nodes that have a higher centrality value than the threshold\n",
        "    for node in list_nodes:\n",
        "            x = airportsDF.loc[airportsDF['IATA'] == node]\n",
        "            x['color'] = 1 if infected_dic[node] else 0\n",
        "            x['markersize'] = 10\n",
        "            newAirportsDF = pd.concat([newAirportsDF, x])\n",
        "\n",
        "    geometry = [Point(xy) for xy in zip(newAirportsDF['lon'], newAirportsDF['lat'])]\n",
        "    gdf = GeoDataFrame(newAirportsDF, geometry=geometry)\n",
        "\n",
        "    # #this is a simple map that goes with geopandas\n",
        "    world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
        "    gdf.plot(column=\"color\", ax=world.plot(figsize=(20, 12)), marker='o', markersize=\"markersize\", cmap='bwr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotSI(infected, susceptible):\n",
        "    days = list(range(len(infected)))\n",
        "    plt.figure(2)\n",
        "    plt.plot(days, infected, label=\"Infected\", color=\"red\")\n",
        "    plt.plot(days, susceptible, label=\"Susceptible\", color=\"blue\")\n",
        "    plt.xlabel(\"Days\")\n",
        "    plt.ylabel(\"Number of people\")\n",
        "    plt.legend()\n",
        "    plt.margins(x=0, y=0)\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute SI step\n",
        "def SI_step(network):\n",
        "    for node in network.nodes:\n",
        "        if network.nodes[node]['infected'] and network.nodes[node]['active']:\n",
        "            for neighbor in network.neighbors(node):\n",
        "                if not network.nodes[neighbor]['infected']:\n",
        "                    if np.random.random() < network.nodes[node]['beta']:\n",
        "                        network.nodes[neighbor]['infected'] = True\n",
        "\n",
        "# compute SI model\n",
        "def SI_model(network, dc_sorted, nSteps, starterNode, theDay, airportPercentage):\n",
        "    susceptible = []\n",
        "    infected = []\n",
        "    network.nodes[starterNode]['infected'] = True\n",
        "    for step in range(nSteps):\n",
        "        # remove nodes based on degree centrality at the Day\n",
        "        if(step == theDay):\n",
        "            # get the top 20% of nodes based on degree centrality\n",
        "            closedAirports = dc_sorted[0:int(len(dc_sorted)*airportPercentage)]\n",
        "            # remove the nodes that are not in the top 20%\n",
        "            for node in network.nodes:\n",
        "                if node in closedAirports:\n",
        "                    network.nodes[node]['beta'] = 0.0001\n",
        "\n",
        "        susceptible.append(len([node for node in network.nodes if not network.nodes[node]['infected']]))\n",
        "        infected.append(len([node for node in network.nodes if network.nodes[node]['infected']]))\n",
        "        SI_step(network)\n",
        "    \n",
        "    return susceptible, infected\n",
        "\n",
        "def full_SI_model(graph, degree_centrality, nSteps, starterNode, theDay, airportPercentage):\n",
        "    network = graph.copy()\n",
        "    # get largest strongly connected component\n",
        "    sccs = nx.strongly_connected_components(network)\n",
        "    sccs_graphs = (network.subgraph(c) for c in sccs)\n",
        "    network = list(sccs_graphs)[0]\n",
        "\n",
        "    # sorted degree centrality\n",
        "    dc_sorted = sorted(degree_centrality, key=dc.get, reverse=True)\n",
        "\n",
        "    # add infection and recovery rate to nodes\n",
        "    for node in network.nodes:\n",
        "        network.nodes[node]['beta'] = 0.01\n",
        "        network.nodes[node]['infected'] = False\n",
        "        network.nodes[node]['active'] = True\n",
        "\n",
        "    susceptible, infected = SI_model(network, dc_sorted, nSteps, starterNode, theDay, airportPercentage)\n",
        "    return susceptible, infected, network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model without preventive measures\n",
        "susceptible, infected, network = full_SI_model(G, dc, 100, 'LIS', 0, 0)\n",
        "days = list(range(len(infected)))\n",
        "\n",
        "infected_dic = {node:network.nodes[node]['infected'] for node in network.nodes}\n",
        "\n",
        "print(infected[len(infected)-1])\n",
        "print(susceptible[len(susceptible)-1])\n",
        "\n",
        "# model with preventive measures\n",
        "susceptible_prev, infected_prev, network_prev = full_SI_model(G, dc, 100, 'LIS', 7, 0.02)\n",
        "infected_dic_prev = {node:network_prev.nodes[node]['infected'] for node in network_prev.nodes}\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(days, infected, label=\"Infected\", color=\"red\")\n",
        "plt.plot(days, susceptible, label=\"Susceptible\", color=\"blue\")\n",
        "plt.plot(days, infected_prev, label=\"Infected (prev. measures)\", color=\"red\", linestyle='dashed')\n",
        "plt.plot(days, susceptible_prev, label=\"Susceptible (prev. measures)\", color=\"blue\", linestyle='dashed')\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Number of people\")\n",
        "plt.legend()\n",
        "plt.margins(x=0, y=0)\n",
        "plt.grid()\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(days, infected, label=\"Infected\", color=\"red\")\n",
        "plt.plot(days, susceptible, label=\"Susceptible\", color=\"blue\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Number of people\")\n",
        "plt.legend()\n",
        "plt.margins(x=0, y=0)\n",
        "plt.grid()\n",
        "\n",
        "plt.figure(3)\n",
        "plt.plot(days, infected_prev, label=\"Infected\", color=\"red\")\n",
        "plt.plot(days, susceptible_prev, label=\"Susceptible\", color=\"blue\")\n",
        "plt.xlabel(\"Days\")\n",
        "plt.ylabel(\"Number of people\")\n",
        "plt.legend()\n",
        "plt.margins(x=0, y=0)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(infected_prev[len(infected)-1])\n",
        "print(susceptible_prev[len(susceptible)-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
